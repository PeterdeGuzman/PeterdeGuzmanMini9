{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PeterdeGuzman/PeterdeGuzmanMini9/blob/main/PeterdeGuzman_Mini9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iszOIeAjEtz"
   },
   "source": [
    "# IDS 706 - Mini Project 9\n",
    "#Cloud-Hosted Notebook Data Manipulation\n",
    "\n",
    "## Peter de Guzman\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2mLXz9ajSao"
   },
   "source": [
    "### Project Explanation:\n",
    "\n",
    "To explore the functionality of a Jupyter notebook hosted through Google Colab, I performed data cleaning operations on a dataset of U.S. counties from the U.S. Census Bureau to perform a crosswalk file that can be used for future data merging. This is a common task for analysts and researchers that use aggregated data for large or medium-size geographic areas in the United States.\n",
    "\n",
    "### Project Requirements:\n",
    "\n",
    "Requirements:\n",
    "* Set up a cloud-hosted Jupyter Notebook (e.g., Google Colab)\n",
    "* Perform data manipulation tasks on a sample dataset\n",
    "* Test the operations in the Jupyter notebook as part of a CI/CD pipeline with GitHub Actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djzZSkHxbLjh"
   },
   "source": [
    "First, we will load the data on U.S. counties from the U.S. Census Bureau. This dataset includes county and state names as well as two-digit state FIPS and three-digit county FIPS codes for each county in the U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vN6EZEXHjPdc"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "raw = pd.read_csv(\n",
    "    \"https://www2.census.gov/geo/docs/reference/codes2020/national_county2020.txt\",\n",
    "    delimiter=\"|\",\n",
    "    header=0,\n",
    "    dtype={\"STATEFP\": str, \"COUNTYFP\": str},\n",
    ")\n",
    "\n",
    "raw_coln = len(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqpDR9Kkb8L9"
   },
   "source": [
    "We will create multiple new fields in this dataset to support future merging operations. At each stage, we will test that the new created variables do not have missing rows and are successfully created.\n",
    "\n",
    "The first variable we will create ia s 5-digit FIPS field by concatenating the two-digit State FIPS codes with the three-digit County FIPS codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "801v3YHl5d-P"
   },
   "outputs": [],
   "source": [
    "raw[\"countyfips\"] = raw[\"STATEFP\"] + raw[\"COUNTYFP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEt2B6MNdyMF"
   },
   "source": [
    "We will add multiple assert statements to ensure that the new field was created successfully without adding missing data, creating invalid codes, or dropping any rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BxRiqJLueAGj"
   },
   "outputs": [],
   "source": [
    "# assert there are no missing values\n",
    "assert raw[\"countyfips\"].isnull().sum() == 0\n",
    "# assert that every 5 digit County FIPS field has 5 digits\n",
    "assert all(\n",
    "    raw[\"countyfips\"].apply(lambda x: len(x) == 5 and x.isdigit())\n",
    "), \"Not all values are 5 digits\"\n",
    "# assert that the number of County Fips values has not changed\n",
    "assert raw_coln == len(raw[\"countyfips\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4rPmgqdeIUe"
   },
   "source": [
    "Some datasets do not include FIPS codes, but instead contain a field with county and state names such as \"Durham County, NC\". To join with these datasets, we will create a matching column for this crosswalk dataset. If there are differences in capitalization, you could use this column in the future by converting this column and its matching field into lowercase before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sZbzzAaqglEi"
   },
   "outputs": [],
   "source": [
    "# Create full County and State names field\n",
    "raw[\"countynames_full\"] = raw[\"COUNTYNAME\"] + \", \" + raw[\"STATE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-_4tRsdgolX"
   },
   "source": [
    "We will again add multiple assert statements to ensure that the new field was created successfully without adding missing data, creating invalid codes, or dropping any rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LssH6d5qiALH"
   },
   "outputs": [],
   "source": [
    "# assert that there are no missing values\n",
    "assert raw_coln == len(raw)\n",
    "\n",
    "# assert that there are no missing values\n",
    "assert raw[\"countynames_full\"].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "214QmjO0iIng"
   },
   "source": [
    "Unfortunately, many national datasets in the U.S. do not include data on Puerto Rico and other American territories. Therefore, we often need to filter out these territories and ensure that each row corresponds with a valid entry for the 50 states and Washington, D.C. The below code filters the dataset, checks that the appropriate rows were dropped, and then asserts that each row has a corresponding state value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_d9MsuSinsH",
    "outputId": "bcb392a6-b030-43f0-ba0b-2a55b5dcc767"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/81/w_61xz297rv4ggdktb58tlxm0000gn/T/ipykernel_35150/3864709302.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.loc[:, \"Valid_State\"] = filtered_df[\"countynames_full\"].apply(\n"
     ]
    }
   ],
   "source": [
    "# Filtering out U.S. territories that are not recognized states\n",
    "# and are not needed for our analysis\n",
    "# Puerto Rico, American Samoa, Guam,\n",
    "# Northern Mariana Islands,\n",
    "# United States Minor Outlying Islands\n",
    "not_states = {\"MP\", \"AS\", \"GU\", \"PR\", \"UM\", \"VI\"}\n",
    "\n",
    "# create regex pattern for checking\n",
    "pattern = \"|\".join(not_states)\n",
    "invalid_counties = raw[raw[\"STATE\"].str.contains(pattern, na=False)].shape[0]\n",
    "\n",
    "filtered_df = raw[~raw[\"STATE\"].isin(not_states)].copy()\n",
    "\n",
    "# Asserting that the rows were dropped\n",
    "assert len(filtered_df) == raw_coln - invalid_counties\n",
    "\n",
    "# assert that there every field has a state associated with it\n",
    "state_abbr = {\n",
    "    \"AL\",  # Alabama\n",
    "    \"AK\",  # Alaska\n",
    "    \"AZ\",  # Arizona\n",
    "    \"AR\",  # Arkansas\n",
    "    \"CA\",  # California\n",
    "    \"CO\",  # Colorado\n",
    "    \"CT\",  # Connecticut\n",
    "    \"DE\",  # Delaware\n",
    "    \"DC\",  # Washington, DC\n",
    "    \"FL\",  # Florida\n",
    "    \"GA\",  # Georgia\n",
    "    \"HI\",  # Hawaii\n",
    "    \"ID\",  # Idaho\n",
    "    \"IL\",  # Illinois\n",
    "    \"IN\",  # Indiana\n",
    "    \"IA\",  # Iowa\n",
    "    \"KS\",  # Kansas\n",
    "    \"KY\",  # Kentucky\n",
    "    \"LA\",  # Louisiana\n",
    "    \"ME\",  # Maine\n",
    "    \"MD\",  # Maryland\n",
    "    \"MA\",  # Massachusetts\n",
    "    \"MI\",  # Michigan\n",
    "    \"MN\",  # Minnesota\n",
    "    \"MS\",  # Mississippi\n",
    "    \"MO\",  # Missouri\n",
    "    \"MT\",  # Montana\n",
    "    \"NE\",  # Nebraska\n",
    "    \"NV\",  # Nevada\n",
    "    \"NH\",  # New Hampshire\n",
    "    \"NJ\",  # New Jersey\n",
    "    \"NM\",  # New Mexico\n",
    "    \"NY\",  # New York\n",
    "    \"NC\",  # North Carolina\n",
    "    \"ND\",  # North Dakota\n",
    "    \"OH\",  # Ohio\n",
    "    \"OK\",  # Oklahoma\n",
    "    \"OR\",  # Oregon\n",
    "    \"PA\",  # Pennsylvania\n",
    "    \"RI\",  # Rhode Island\n",
    "    \"SC\",  # South Carolina\n",
    "    \"SD\",  # South Dakota\n",
    "    \"TN\",  # Tennessee\n",
    "    \"TX\",  # Texas\n",
    "    \"UT\",  # Utah\n",
    "    \"VT\",  # Vermont\n",
    "    \"VA\",  # Virginia\n",
    "    \"WA\",  # Washington\n",
    "    \"WV\",  # West Virginia\n",
    "    \"WI\",  # Wisconsin\n",
    "    \"WY\",  # Wyoming\n",
    "}\n",
    "\n",
    "# Use a lambda function to check the last two letters\n",
    "filtered_df.loc[:, \"Valid_State\"] = filtered_df[\"countynames_full\"].apply(\n",
    "    lambda county_state: county_state.split(\", \")[-1] in state_abbr\n",
    ")\n",
    "\n",
    "\n",
    "# Assert that all entries are valid\n",
    "assert all(\n",
    "    filtered_df[\"Valid_State\"]\n",
    "), \"Some entries do not have valid state abbreviations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC2fljFfisdL"
   },
   "source": [
    "Finally, we will save the cleaned and filtered crosswalk dataset to a csv file for use in future data manipulation and merging operations. For this example, we will save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Y3LfhLgbizrF"
   },
   "outputs": [],
   "source": [
    "# Saving output to \"00 - Resources\"\n",
    "filtered_df.to_csv(\n",
    "    \"fips_xwalk.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2I66GFgjlx3"
   },
   "source": [
    "Finally, we will assert that the file was successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ipw2xsSdjpS3"
   },
   "outputs": [],
   "source": [
    "file_path = \"fips_xwalk.csv\"\n",
    "assert os.path.exists(file_path), \"File does not exist.\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkZjXayVai/yLrEHdbLfBq",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
